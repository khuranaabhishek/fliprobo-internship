{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f5e9cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72991f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Title  Rating  Year\n",
      "0                     Ship of Theseus     8.0  2012\n",
      "1                              Iruvar     8.4  1997\n",
      "2                     Kaagaz Ke Phool     7.8  1959\n",
      "3   Lagaan: Once Upon a Time in India     8.1  2001\n",
      "4                     Pather Panchali     8.2  1955\n",
      "5                           Charulata     8.1  1964\n",
      "6                     Rang De Basanti     8.1  2006\n",
      "7                               Dev.D     7.9  2009\n",
      "8                            3 Idiots     8.4  2009\n",
      "9                              Awaara     7.8  1951\n",
      "10                            Nayakan     8.7  1987\n",
      "11                          Aparajito     8.2  1956\n",
      "12                    Pushpaka Vimana     8.6  1987\n",
      "13                             Pyaasa     8.3  1957\n",
      "14                      Ghatashraddha     7.5  1977\n",
      "15                             Sholay     8.1  1975\n",
      "16                           Aradhana     7.6  1969\n",
      "17              Do Ankhen Barah Haath     8.4  1957\n",
      "18                             Bombay     8.1  1995\n",
      "19                       Neecha Nagar     6.6  1946\n",
      "20                     Do Bigha Zamin     8.3  1953\n",
      "21                          Garm Hava     8.0  1974\n",
      "22                             Piravi     7.7  1989\n",
      "23                      Mughal-E-Azam     8.1  1960\n",
      "24                        Amma Ariyan     7.4  1986\n",
      "Data has been saved to 'top_100_indian_movies_list.csv'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "\n",
    "driver.get('https://www.imdb.com/list/ls056092300/')\n",
    "\n",
    "\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    \n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    \n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "\n",
    "script_tag = soup.find('script', type='application/ld+json')\n",
    "\n",
    "ratings = {}\n",
    "if script_tag:\n",
    "    json_data = json.loads(script_tag.string)\n",
    "    for item in json_data.get('itemListElement', []):\n",
    "        movie = item.get('item', {})\n",
    "        title = movie.get('name', 'N/A').strip()\n",
    "        rating = movie.get('aggregateRating', {}).get('ratingValue', 'N/A')\n",
    "        ratings[title] = rating\n",
    "\n",
    "\n",
    "movie_containers = soup.find_all('div', class_='ipc-title ipc-title--base ipc-title--title ipc-title-link-no-icon ipc-title--on-textPrimary sc-b189961a-9 bnSrml dli-title')\n",
    "\n",
    "movies = []\n",
    "\n",
    "for container in movie_containers:\n",
    "    \n",
    "    raw_title = container.find('h3', class_='ipc-title__text').text.strip()\n",
    "    title = ' '.join(raw_title.split(' ', 1)[1:]).strip()  \n",
    "\n",
    "    \n",
    "    metadata_container = container.find_next_sibling('div', class_='sc-b189961a-7 btCcOY dli-title-metadata')\n",
    "    year = 'N/A'\n",
    "    if metadata_container:\n",
    "        metadata_items = metadata_container.find_all('span', class_='sc-b189961a-8 hCbzGp dli-title-metadata-item')\n",
    "        if len(metadata_items) > 0:\n",
    "            year = metadata_items[0].text.strip()\n",
    "\n",
    "    \n",
    "    normalized_title = title.strip()\n",
    "\n",
    "    \n",
    "    rating = ratings.get(normalized_title, 'N/A')\n",
    "\n",
    "    movies.append([title, rating, year])\n",
    "\n",
    "movies_df = pd.DataFrame(movies, columns=['Title', 'Rating', 'Year'])\n",
    "print(movies_df.head(100))\n",
    "movies_df.to_csv('top_100_indian_movies_list.csv', index=False)\n",
    "print(\"Data has been saved to 'top_100_indian_movies_list.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23d8231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "352d6384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Heading          Date  \\\n",
      "0  Python Tkinter Tutorial (Part 1): Getting Star...       July 28   \n",
      "1  Automating My Bill Payments with Python and Se...        June 1   \n",
      "2  Python Tutorial: Securely Manage Passwords and...      April 30   \n",
      "3  Automate Your Development Environment Setup wi...      April 30   \n",
      "4  How to Use ChatGPT as a Powerful Tool for Prog...  May 21, 2023   \n",
      "\n",
      "                                             Content Likes  \n",
      "0                                                      N/A  \n",
      "1                                                      N/A  \n",
      "2  In this Python Programming video, we will be l...   N/A  \n",
      "3  In this video, we'll be learning how I set up ...   N/A  \n",
      "4                                                      N/A  \n",
      "Data has been saved to 'patreon_posts.csv'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "def scrape_patreon():\n",
    "    url = 'https://www.patreon.com/coreyms'\n",
    "    \n",
    "    \n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    driver.get(url)\n",
    "    \n",
    "   \n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  \n",
    "        \n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    \n",
    "    \n",
    "    page_source = driver.page_source\n",
    "    \n",
    "    \n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    \n",
    "    posts = []\n",
    "    post_containers = soup.find_all('div', class_='sc-xg7a7d-0 jJOVvL')\n",
    "    \n",
    "    for post in post_containers:\n",
    "        heading = post.find('div', class_='sc-bBHxTw jIEOUn').get_text(strip=True) if post.find('div', class_='sc-bBHxTw jIEOUn') else 'N/A'\n",
    "        \n",
    "        \n",
    "        date = post.find('div', class_='sc-lgu5zg-0 gDgFXW').get_text(strip=True).replace('New', '').strip() if post.find('div', class_='sc-lgu5zg-0 gDgFXW') else 'N/A'\n",
    "        content = post.find('div', class_='sc-cfnzm4-0 daxSFj').get_text(strip=True) if post.find('div', class_='sc-cfnzm4-0 daxSFj') else 'N/A'\n",
    "        \n",
    "        youtube_url = None\n",
    "        video_iframe = post.find('iframe')\n",
    "        if video_iframe and 'youtube.com' in video_iframe['src']:\n",
    "            youtube_url = video_iframe['src']\n",
    "            youtube_url = re.sub(r'&amp;.*$', '', youtube_url)  \n",
    "\n",
    "        likes = 'N/A'\n",
    "        if youtube_url:\n",
    "            likes = get_youtube_likes(driver, youtube_url) \n",
    "        \n",
    "        posts.append([heading, date, content, likes])\n",
    "    \n",
    "   \n",
    "    posts_df = pd.DataFrame(posts, columns=['Heading', 'Date', 'Content', 'Likes'])\n",
    "    \n",
    "   \n",
    "    print(posts_df.head())\n",
    "    \n",
    "    \n",
    "    posts_df.to_csv('patreon_posts.csv', index=False)\n",
    "    print(\"Data has been saved to 'patreon_posts.csv'\")\n",
    "    \n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_patreon()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee11aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e17f1dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title             Location  \\\n",
      "0  2 BHK Flat In Ramya Regent For Sale  In Indira...  near police station   \n",
      "\n",
      "              Area                         EMI     Price  \n",
      "0  917 sqftBuiltup  ₹1 Crore₹10,905 per sq.ft.  ₹1 Crore  \n",
      "Data has been saved to 'nobroker_house_details.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = 'https://www.nobroker.in/property/sale/bangalore/multiple?searchParam=W3sibGF0IjoxMi45NzgzNjkyLCJsb24iOjc3LjY0MDgzNTYsInBsYWNlSWQiOiJDaElKa1FOM0dLUVdyanNSTmhCUUpyaEdEN1UiLCJwbGFjZU5hbWUiOiJJbmRpcmFuYWdhciJ9LHsibGF0IjoxMi45MzA3NzM1LCJsb24iOjc3LjU4MzgzMDIsInBsYWNlSWQiOiJDaElKMmRkbFo1Z1ZyanNSaDFCT0FhZi1vcnMiLCJwbGFjZU5hbWUiOiJKYXlhbmFnYXIifSx7ImxhdCI6MTIuOTk4MTczMiwibG9uIjo3Ny41NTMwNDQ1OTk5OTk5OSwicGxhY2VJZCI6IkNoSUp4Zlc0RFBNOXJqc1JLc05URy01cF9RUSIsInBsYWNlTmFtZSI6IlJhamFqaW5hZ2FyIn1d&radius=2.0&city=bangalore&locality=Indiranagar,Jayanagar,Rajajinagar'  # Example URL\n",
    "\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "\n",
    "def scrape_houses(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "\n",
    "    listings = soup.find_all('div', class_='infinite-scroll-component__outerdiv')  \n",
    "\n",
    "    houses = []\n",
    "    \n",
    "    for listing in listings:\n",
    "        try:\n",
    "            \n",
    "            title = listing.find('h2', class_='heading-6 flex items-center font-semi-bold m-0').text.strip()\n",
    "\n",
    "            location = listing.find('div', class_='mt-0.5p overflow-hidden overflow-ellipsis whitespace-nowrap max-w-70 text-gray-light leading-4 po:mb-0.1p po:max-w-95').text.strip()\n",
    "\n",
    "            area = listing.find('div', class_='flex flex-col w-33pe items-center tp:w-half po:w-full').text.strip()\n",
    "\n",
    "            emi = listing.find('div', class_='flex flex-col w-33pe items-center bo tp:w-half po:w-full border-r border-r-solid border-card-overview-border-color last:border-r-1').text.strip()\n",
    "\n",
    "            price = listing.find('div', class_='font-semi-bold heading-6').text.strip()\n",
    "\n",
    "            \n",
    "            houses.append({\n",
    "                'Title': title,\n",
    "                'Location': location,\n",
    "                'Area': area,\n",
    "                'EMI': emi,\n",
    "                'Price': price\n",
    "            })\n",
    "        except AttributeError:\n",
    "            \n",
    "            continue\n",
    "    \n",
    "    return houses\n",
    "\n",
    "\n",
    "houses = scrape_houses(url)\n",
    "houses_df = pd.DataFrame(houses)\n",
    "print(houses_df)\n",
    "houses_df.to_csv('nobroker_house_details.csv', index=False)\n",
    "print(\"Data has been saved to 'nobroker_house_details.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba093cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "988da169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Product Name Price  \\\n",
      "0          bewakoof x tom & jerry  ₹499   \n",
      "1                   bewakoof x dc  ₹499   \n",
      "2             bewakoof x garfield  ₹549   \n",
      "3  bewakoof x house of the dragon  ₹399   \n",
      "4                       Bewakoof®  ₹349   \n",
      "5                       Bewakoof®  ₹439   \n",
      "6                       Bewakoof®  ₹399   \n",
      "7                       Bewakoof®  ₹499   \n",
      "8                 bewakoof x nasa  ₹399   \n",
      "9               bewakoof x marvel  ₹599   \n",
      "\n",
      "                                           Image URL  \n",
      "0  https://images.bewakoof.com/t640/women-s-blue-...  \n",
      "1  https://images.bewakoof.com/t640/men-s-black-a...  \n",
      "2  https://images.bewakoof.com/t640/women-s-green...  \n",
      "3  https://images.bewakoof.com/t640/men-s-black-h...  \n",
      "4  https://images.bewakoof.com/t640/women-s-red-i...  \n",
      "5  https://images.bewakoof.com/t640/women-s-brown...  \n",
      "6  https://images.bewakoof.com/t640/women-s-pink-...  \n",
      "7  https://images.bewakoof.com/t640/men-s-sun-kis...  \n",
      "8  https://images.bewakoof.com/t640/men-s-red-nas...  \n",
      "9  https://images.bewakoof.com/t640/men-s-black-g...  \n",
      "Data has been saved to 'bewakoof_product_details.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = 'https://www.bewakoof.com/bestseller?sort=popular'\n",
    "\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "product_containers = soup.find_all('div', class_='productCardBox', limit=10)\n",
    "\n",
    "\n",
    "products = []\n",
    "\n",
    "\n",
    "for container in product_containers:\n",
    "    try:\n",
    "        \n",
    "        name = container.find('h3').text.strip()\n",
    "        \n",
    "        \n",
    "        price = container.find('div', class_='discountedPriceText clr-p-black false').text.strip()\n",
    "        \n",
    "        \n",
    "        image_url = container.find('img')['src']\n",
    "        \n",
    "        \n",
    "        products.append({\n",
    "            'Product Name': name,\n",
    "            'Price': price,\n",
    "            'Image URL': image_url\n",
    "        })\n",
    "    except AttributeError:\n",
    "        continue\n",
    "\n",
    "products_df = pd.DataFrame(products)\n",
    "print(products_df)\n",
    "products_df.to_csv('bewakoof_product_details.csv', index=False)\n",
    "print(\"Data has been saved to 'bewakoof_product_details.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a5e17ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c715e301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Heading             Date  \\\n",
      "0   How Americans are losing their life savings to...       39 Min Ago   \n",
      "1   Good economic data sparked a stock rally. Here...      4 Hours Ago   \n",
      "2   With Harris economic plan, U.S. family and chi...      4 Hours Ago   \n",
      "3   'Couples Therapy' host Orna Guralnik doesn't t...      4 Hours Ago   \n",
      "4   This 37-year-old brings in more than $100K per...      4 Hours Ago   \n",
      "5   3 side hustles for teachers—one can pay as muc...      5 Hours Ago   \n",
      "6   Big fast-casual restaurant chains are rapidly ...      5 Hours Ago   \n",
      "7   To fix Starbucks, incoming CEO Niccol will hav...      5 Hours Ago   \n",
      "8   Former Google researcher's company wants to gi...      5 Hours Ago   \n",
      "9   5 money moves to consider ahead of the Fed's f...      5 Hours Ago   \n",
      "10  Chinese companies are already investing in the...      5 Hours Ago   \n",
      "11  AI is unlocking new uses for military drones, ...      5 Hours Ago   \n",
      "12  Stablecoins are growing but not as a share of ...      5 Hours Ago   \n",
      "13  Berkshire Hathaway's new stock pick Ulta Beaut...      5 Hours Ago   \n",
      "14  Here's why the Walgreens and CVS retail pharma...      6 Hours Ago   \n",
      "15  Harris campaign launches $370 million fall ad ...     23 Hours Ago   \n",
      "16  Biden to make a forceful case for Harris and W...  August 17, 2024   \n",
      "17  3 ways your body lets you know you're experien...  August 17, 2024   \n",
      "18  5 tips from happiness experts on how to improv...  August 17, 2024   \n",
      "19  Why NASA astronauts are waiting to return on B...  August 14, 2024   \n",
      "20  How to determine your risk appetite for an inv...  August 17, 2024   \n",
      "21  The 10 most-expensive cities to raise kids in ...  August 17, 2024   \n",
      "22  34-year-old's summer side hustle is working as...  August 17, 2024   \n",
      "23  This skill made Netflix worth $289 billion, sa...  August 17, 2024   \n",
      "24  Mom co-signed daughter's student loan. Now 85,...  August 17, 2024   \n",
      "25  The 10 most popular summer travel destinations...  August 17, 2024   \n",
      "26  The first SpaceX spacewalk: What Polaris Dawn'...  August 17, 2024   \n",
      "27  How a week's worth of huge economic news will ...  August 17, 2024   \n",
      "28  Egg prices are rising once again as bird flu l...  August 17, 2024   \n",
      "29  Bernstein tells clients to buy these favorite ...  August 17, 2024   \n",
      "\n",
      "                                                 Link  \n",
      "0   https://www.cnbc.comhttps://www.cnbc.com/2024/...  \n",
      "1   https://www.cnbc.comhttps://www.cnbc.com/2024/...  \n",
      "2   https://www.cnbc.comhttps://www.cnbc.com/2024/...  \n",
      "3   https://www.cnbc.comhttps://www.cnbc.com/2024/...  \n",
      "4   https://www.cnbc.comhttps://www.cnbc.com/2024/...  \n",
      "5   https://www.cnbc.comhttps://www.cnbc.com/2024/...  \n",
      "6   https://www.cnbc.comhttps://www.cnbc.com/2024/...  \n",
      "7   https://www.cnbc.comhttps://www.cnbc.com/2024/...  \n",
      "8   https://www.cnbc.comhttps://www.cnbc.com/2024/...  \n",
      "9   https://www.cnbc.comhttps://www.cnbc.com/2024/...  \n",
      "10  https://www.cnbc.comhttps://www.cnbc.com/2024/...  \n",
      "11  https://www.cnbc.comhttps://www.cnbc.com/2024/...  \n",
      "12  https://www.cnbc.comhttps://www.cnbc.com/2024/...  \n",
      "13  https://www.cnbc.comhttps://www.cnbc.com/2024/...  \n",
      "14  https://www.cnbc.comhttps://www.cnbc.com/2024/...  \n",
      "15  https://www.cnbc.comhttps://www.cnbc.com/2024/...  \n",
      "16  https://www.cnbc.comhttps://www.cnbc.com/2024/...  \n",
      "17  https://www.cnbc.comhttps://www.cnbc.com/2024/...  \n",
      "18  https://www.cnbc.comhttps://www.cnbc.com/2024/...  \n",
      "19  https://www.cnbc.comhttps://www.cnbc.com/2024/...  \n",
      "20  https://www.cnbc.comhttps://www.cnbc.com/2024/...  \n",
      "21  https://www.cnbc.comhttps://www.cnbc.com/2024/...  \n",
      "22  https://www.cnbc.comhttps://www.cnbc.com/2024/...  \n",
      "23  https://www.cnbc.comhttps://www.cnbc.com/2024/...  \n",
      "24  https://www.cnbc.comhttps://www.cnbc.com/2024/...  \n",
      "25  https://www.cnbc.comhttps://www.cnbc.com/2024/...  \n",
      "26  https://www.cnbc.comhttps://www.cnbc.com/2024/...  \n",
      "27  https://www.cnbc.comhttps://www.cnbc.com/2024/...  \n",
      "28  https://www.cnbc.comhttps://www.cnbc.com/2024/...  \n",
      "29  https://www.cnbc.comhttps://www.cnbc.com/2024/...  \n",
      "Data has been saved to 'cnbc_world_news.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "articles = soup.find_all('div', class_='LatestNews-container')\n",
    "\n",
    "\n",
    "news_data = []\n",
    "\n",
    "\n",
    "for article in articles:\n",
    "    try:\n",
    "        \n",
    "        heading = article.find('a', class_='LatestNews-headline').text.strip()\n",
    "        \n",
    "        \n",
    "        date = article.find('time', class_='LatestNews-timestamp').text.strip()\n",
    "        \n",
    "        \n",
    "        news_link = article.find('a', class_='LatestNews-headline')['href']\n",
    "        news_link = 'https://www.cnbc.com' + news_link\n",
    "        news_data.append({\n",
    "            'Heading': heading,\n",
    "            'Date': date,\n",
    "            'Link': news_link\n",
    "        })\n",
    "    except AttributeError:\n",
    "        continue\n",
    "\n",
    "news_df = pd.DataFrame(news_data)\n",
    "print(news_df)\n",
    "news_df.to_csv('cnbc_world_news.csv', index=False)\n",
    "print(\"Data has been saved to 'cnbc_world_news.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "081708a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08a40ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Title            Date  \\\n",
      "0   Implementation of artificial intelligence in a...            2020   \n",
      "1               Review of agricultural IoT technology            2022   \n",
      "2   Automation and digitization of agriculture usi...            2021   \n",
      "3   A comprehensive review on automation in agricu...       June 2019   \n",
      "4   Applications of electronic nose (e-nose) and e...            2020   \n",
      "5   Towards sustainable agriculture: Harnessing AI...       June 2024   \n",
      "6             Fruit ripeness classification: A survey      March 2023   \n",
      "7   Deep learning based computer vision approaches...            2022   \n",
      "8   A review of imaging techniques for plant disea...            2020   \n",
      "9   Transfer Learning for Multi-Crop Leaf Disease ...            2022   \n",
      "10  Comparison of CNN-based deep learning architec...  September 2023   \n",
      "11  DeepRice: A deep learning and deep feature bas...      March 2024   \n",
      "12  Computer vision in smart agriculture and preci...  September 2024   \n",
      "13  Image classification on smart agriculture plat...  September 2024   \n",
      "14  Using an improved lightweight YOLOv8 model for...      March 2024   \n",
      "15  How artificial intelligence uses to achieve th...       June 2023   \n",
      "16  LeafSpotNet: A deep learning framework for det...       June 2024   \n",
      "17  Plant disease detection using hybrid model bas...            2021   \n",
      "18  Cross-comparative review of Machine learning f...       June 2024   \n",
      "19  A comprehensive survey on weed and crop classi...  September 2024   \n",
      "20  A systematic review of machine learning techni...            2022   \n",
      "21  Machine learning in nutrient management: A review  September 2023   \n",
      "22  Deep convolutional neural network models for w...            2022   \n",
      "23  A review on computer vision systems in monitor...            2020   \n",
      "24  Explainable artificial intelligence and interp...            2022   \n",
      "\n",
      "                                               Author  \n",
      "0   Tanha Talaviya |  Dhara Shah |  Nivedita Patel...  \n",
      "1          Jinyuan Xu |  Baoxing Gu |  Guangzhao Tian  \n",
      "2                            A. Subeesh |  C.R. Mehta  \n",
      "3   Kirtan Jha |  Aalap Doshi |  Poojan Patel |  M...  \n",
      "4                               Juzhong Tan |  Jie Xu  \n",
      "5                 Dhananjay K. Pandey |  Richa Mishra  \n",
      "6   Matteo Rizzo |  Matteo Marcuzzo |  Alessandro ...  \n",
      "7   V.G. Dhanya |  A. Subeesh |  N.L. Kushwaha |  ...  \n",
      "8        Vijai Singh |  Namita Sharma |  Shikha Singh  \n",
      "9              Ananda S. Paymode |  Vandana B. Malode  \n",
      "10  Md Taimur Ahad |  Yan Li |  Bo Song |  Touhid ...  \n",
      "11  P. Isaac Ritharson |  Kumudha Raimond |  X. An...  \n",
      "12  Sumaira Ghazal |  Arslan Munir |  Waqar S. Qur...  \n",
      "13  Juan Felipe Restrepo-Arias |  John W. Branch-B...  \n",
      "14  Baoling Ma |  Zhixin Hua |  Yuchen Wen |  Hong...  \n",
      "15            Vilani Sachithra |  L.D.C.S. Subhashini  \n",
      "16         Shwetha V |  Arnav Bhagwat |  Vijaya Laxmi  \n",
      "17                         Punam Bedi |  Pushkar Gole  \n",
      "18  James Daniel Omaye |  Emeka Ogbuju |  Grace At...  \n",
      "19  Faisal Dharma Adhinata |   Wahyono |  Raden Su...  \n",
      "20  Md Ekramul Hossain |  Muhammad Ashad Kabir |  ...  \n",
      "21  Oumnia Ennaji |  Leonardus Vergütz |  Achraf E...  \n",
      "22  A. Subeesh |  S. Bhole |  K. Singh |  N.S. Cha...  \n",
      "23  Cedric Okinda |  Innocent Nyalala |  Tchalla K...  \n",
      "24                                       Masahiro Ryo  \n",
      "Data has been saved to 'most_downloaded_articles.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = 'https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded-articles/'\n",
    "\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "papers_data = []\n",
    "\n",
    "\n",
    "articles = soup.find_all('div', class_='article-listing')\n",
    "\n",
    "\n",
    "\n",
    "for article in articles:\n",
    "    try:\n",
    "        \n",
    "        title = article.find('h2').text.strip()\n",
    "\n",
    "        date = article.find('p', class_='article-date').text.strip()\n",
    "\n",
    "        author = article.find('p', class_='article-authors').text.strip()\n",
    "        \n",
    "        \n",
    "        papers_data.append({\n",
    "            'Title': title,\n",
    "            'Date': date,\n",
    "            'Author': author\n",
    "        })\n",
    "    except AttributeError:\n",
    "        continue\n",
    "\n",
    "papers_df = pd.DataFrame(papers_data)\n",
    "print(papers_df)\n",
    "papers_df.to_csv('most_downloaded_articles.csv', index=False)\n",
    "print(\"Data has been saved to 'most_downloaded_articles.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bac905d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
